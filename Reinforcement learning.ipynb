{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Skating\n",
    "\n",
    "> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster than him. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n",
    "\n",
    "First, let's install the gym and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "%pip install gym \n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cartpole environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the environment works, let's run a short simulation for 100 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   env.render()\n",
    "   env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, `step` function returns us back current observations, reward function, and the `done` flag that indicates whether it makes sense to continue the simulation or not:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs = env.step(env.action_space.sample())\n",
    "   rew = env.step(env.action_space.sample())\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get min and max value of those numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x):\n",
    "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore other discretization method using bins:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(i,num):\n",
    "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
    "\n",
    "# print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),20))\n",
    "\n",
    "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
    "nbins = [20,20,10,10] # number of bins for each parameter\n",
    "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
    "print(bins)\n",
    "\n",
    "def discretize_bins(x):\n",
    "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a short simulation and observe those discrete environment values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "steps = 0\n",
    "while not done:\n",
    "   obs = env.step(env.action_space.sample())\n",
    "   print(obs)\n",
    "   if steps % 10 == 0:\n",
    "       print(discretize(obs[0]))\n",
    "   steps += 1\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "actions = (0,1)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Q-Learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.90"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implementing the Q-learning algorithm with epsilon-greedy exploration strategy. The Q-learning algorithm is a popular Reinforcement Learning technique for learning an optimal policy for an agent in a Markov Decision Process (MDP) with a discrete action and state space.\n",
    "\n",
    "In this code, the agent interacts with the environment for 100000 epochs, where in each epoch, it resets the environment, chooses an action, takes a step, and updates its Q-table based on the observed reward and next state. The agent uses an epsilon-greedy exploration strategy to balance the exploration-exploitation trade-off during the learning process.\n",
    "\n",
    "The Q-values are stored in a dictionary called Q, where each key is a tuple of (state, action) pair, and the corresponding value is the estimated Q-value for that state-action pair. The qvalues(s) function is used to get the Q-values for a given state s.\n",
    "\n",
    "The discretize_bins function seems to be used for discretizing continuous state space into a discrete set of states for Q-learning. The probs function is used to calculate the probability distribution over the Q-values for a given state using the softmax function.\n",
    "\n",
    "The cum_rewards list is used to keep track of the cumulative reward obtained in each epoch, and the rewards list is used to keep track of the reward obtained in each step. The alpha and gamma variables are the learning rate and discount factor, respectively, used for updating the Q-values.\n",
    "\n",
    "The code prints the average cumulative reward obtained over the last 5000 epochs and updates the Qmax and Qbest variables if the current average reward is higher than the previous best reward. Overall, this code seems to be a simple implementation of the Q-learning algorithm with an epsilon-greedy exploration strategy for solving MDPs with a discrete action and state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = {}\n",
    "# actions = [0, 1]\n",
    "\n",
    "# def qvalues(state):\n",
    "#     return [Q.get((state, a), 0) for a in actions]\n",
    "\n",
    "def probs(v, eps=1e-4):\n",
    "    v = v - v.min() + eps\n",
    "    v = v / v.sum()\n",
    "    return v\n",
    "\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(100000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        s = discretize(obs[0])\n",
    "        print(s)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            # Exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions, weights=v)[0]\n",
    "        else:\n",
    "            # Exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        print(a)\n",
    "        obs = env.step(a) \n",
    "        rew = env.step(a)\n",
    "        print(rew)\n",
    "        #1.0\n",
    "        cum_reward += rew[1]\n",
    "        ns = discretize(obs[0])\n",
    "        Q[(s, a)] = (1 - alpha) * Q.get((s, a), 0) + alpha * (rew[1] + gamma * max(qvalues(ns)))\n",
    "\n",
    "    cum_rewards.append(cum_reward)\n",
    "    \n",
    "    # Periodically print results and calculate average reward\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Training Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('3.7')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
