{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Lesson 4\n",
    "\n",
    "Load up required libraries and dataset. Convert the data to a dataframe containing a subset of the data\n",
    "## Introduction\n",
    "\n",
    "In this final lesson on Regression, one of the basic _classic_ ML techniques, we will take a look at Logistic Regression. You would use this technique to discover patterns to predict binary categories. Is this candy chocolate or not? Is this disease contagious or not? Will this customer choose this product or not? \n",
    "\n",
    "In this lesson, you will learn:\n",
    "\n",
    "- A new library for data visualization\n",
    "- Techniques for logistic regression\n",
    "\n",
    "âœ… Deepen your understanding of working with this type of regression in this [Learn module](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-15963-cxa)\n",
    "## Prerequisite\n",
    "\n",
    "Having worked with the pumpkin data, we are now familiar enough with it to realize that there's one binary category that we can work with: `Color`.\n",
    "\n",
    "Let's build a logistic regression model to predict that, given some variables, _what color a given pumpkin is likely to be_ (orange ðŸŽƒ or white ðŸ‘»).\n",
    "\n",
    "> Why are we talking about binary classification in a lesson grouping about regression? Only for linguistic convenience, as logistic regression is [really a classification method](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), albeit a linear-based one. Learn about other ways to classify data in the next lesson group.\n",
    "\n",
    "## Define the question\n",
    "\n",
    "For our purposes, we will express this as a binary: 'Orange' or 'Not Orange'. There is also a 'striped' category in our dataset but there are few instances of it, so we will not use it. It disappears once we remove null values from the dataset, anyway.\n",
    "\n",
    "> ðŸŽƒ Fun fact, we sometimes call white pumpkins 'ghost' pumpkins. They aren't very easy to carve, so they aren't as popular as the orange ones but they are cool looking!\n",
    "\n",
    "## About logistic regression\n",
    "\n",
    "Logistic regression differs from linear regression, which you learned about previously, in a few important ways.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "Logistic regression does not offer the same features as linear regression. The former offers a prediction about a binary category (\"orange or not orange\") whereas the latter is capable of predicting continual values, for example given the origin of a pumpkin and the time of harvest, _how much its price will rise_.\n",
    "\n",
    "![Pumpkin classification Model](./images/pumpkin-classifier.png)\n",
    "> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n",
    "### Other classifications\n",
    "\n",
    "There are other types of logistic regression, including multinomial and ordinal:\n",
    "\n",
    "- **Multinomial**, which involves having more than one category - \"Orange, White, and Striped\".\n",
    "- **Ordinal**, which involves ordered categories, useful if we wanted to order our outcomes logically, like our pumpkins that are ordered by a finite number of sizes (mini,sm,med,lg,xl,xxl).\n",
    "\n",
    "![Multinomial vs ordinal regression](./images/multinomial-ordinal.png)\n",
    "> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n",
    "\n",
    "### It's still linear\n",
    "\n",
    "Even though this type of Regression is all about 'category predictions', it still works best when there is a clear linear relationship between the dependent variable (color) and the other independent variables (the rest of the dataset, like city name and size). It's good to get an idea of whether there is any linearity dividing these variables or not.\n",
    "\n",
    "### Variables DO NOT have to correlate\n",
    "\n",
    "Remember how linear regression worked better with more correlated variables? Logistic regression is the opposite - the variables don't have to align. That works for this data which has somewhat weak correlations.\n",
    "\n",
    "### You need a lot of clean data\n",
    "\n",
    "Logistic regression will give more accurate results if you use more data; our small dataset is not optimal for this task, so keep that in mind.\n",
    "\n",
    "âœ… Think about the types of data that would lend themselves well to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pumpkins = pd.read_csv('data/US-pumpkins.csv')\n",
    "\n",
    "pumpkins.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - tidy the data\n",
    "\n",
    "First, clean the data a bit, dropping null values and selecting only some of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "new_columns = ['Color','Origin','Item Size','Variety','City Name','Package']\n",
    "\n",
    "new_pumpkins = pumpkins.drop([c for c in pumpkins.columns if c not in new_columns], axis=1)\n",
    "\n",
    "\n",
    "new_pumpkins.dropna(inplace=True)\n",
    "\n",
    "new_pumpkins = new_pumpkins.apply(LabelEncoder().fit_transform)\n",
    "print(new_pumpkins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data shape, size, and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pumpkins.info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - side-by-side grid\n",
    "\n",
    "By now you have loaded up the [starter notebook](./notebook.ipynb) with pumpkin data once again and cleaned it so as to preserve a dataset containing a few variables, including `Color`. Let's visualize the dataframe in the notebook using a different library: [Seaborn](https://seaborn.pydata.org/index.html), which is built on Matplotlib which we used earlier. \n",
    "\n",
    "Seaborn offers some neat ways to visualize your data. For example, you can compare distributions of the data for each point in a side-by-side grid.\n",
    "\n",
    "1. Create such a grid by instantiating a `PairGrid`, using our pumpkin data `new_pumpkins`, followed by calling `map()`:\n",
    "Working with Item Size to Color, create a scatterplot using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "g = sns.PairGrid(new_pumpkins)\n",
    "g.map(sns.scatterplot)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a swarm plot\n",
    "\n",
    "Since Color is a binary category (Orange or Not), it's called 'categorical data' and needs 'a more [specialized approach](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) to visualization'. There are other ways to visualize the relationship of this category with other variables. \n",
    "\n",
    "You can visualize variables side-by-side with Seaborn plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=\"Color\", y=\"Item Size\", data=new_pumpkins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plot\n",
    "\n",
    "A 'violin' type plot is useful as you can easily visualize the way that data in the two categories is distributed. Violin plots don't work so well with smaller datasets as the distribution is displayed more 'smoothly'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Color\", y=\"Item Size\",\n",
    "            kind=\"violin\", data=new_pumpkins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your model\n",
    "\n",
    "Building a model to find these binary classification is surprisingly straightforward in Scikit-learn.\n",
    "\n",
    "1. Select the variables you want to use in your classification model and split the training and test sets calling `train_test_split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Selected_features = ['Origin','Item Size','Variety','City Name','Package']\n",
    "\n",
    "X = new_pumpkins[Selected_features]\n",
    "y = new_pumpkins['Color']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now you can train your model, by calling `fit()` with your training data, and print out its result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Let's say our model is asked to classify pumpkins between two binary categories, category 'orange' and category 'not-orange'.\n",
    "\n",
    "- If your model predicts a pumpkin as not orange and it belongs to category 'not-orange' in reality we call it a true negative, shown by the top left number.\n",
    "- If your model predicts a pumpkin as orange and it belongs to category 'not-orange' in reality we call it a false negative, shown by the bottom left number. \n",
    "- If your model predicts a pumpkin as not orange and it belongs to category 'orange' in reality we call it a false positive, shown by the top right number. \n",
    "- If your model predicts a pumpkin as orange and it belongs to category 'orange' in reality we call it a true positive, shown by the bottom right number.\n",
    "\n",
    "As you might have guessed it's preferable to have a larger number of true positives and true negatives and a lower number of false positives and false negatives, which implies that the model performs better.\n",
    "\n",
    "How does the confusion matrix relate to precision and recall? Remember, the classification report printed above showed precision (0.83) and recall (0.98).\n",
    "\n",
    "Precision = tp / (tp + fp) = 162 / (162 + 33) = 0.8307692307692308\n",
    "\n",
    "Recall = tp / (tp + fn) = 162 / (162 + 4) = 0.9759036144578314\n",
    "\n",
    "âœ… Q: According to the confusion matrix, how did the model do? A: Not too bad; there are a good number of true negatives but also several false negatives. \n",
    "\n",
    "Let's revisit the terms we saw earlier with the help of the confusion matrix's mapping of TP/TN and FP/FN:\n",
    "\n",
    "ðŸŽ“ Precision: TP/(TP + FP) The fraction of relevant instances among the retrieved instances (e.g. which labels were well-labeled)\n",
    "\n",
    "ðŸŽ“ Recall: TP/(TP + FN) The fraction of relevant instances that were retrieved, whether well-labeled or not\n",
    "\n",
    "ðŸŽ“ f1-score: (2 * precision * recall)/(precision + recall) A weighted average of the precision and recall, with best being 1 and worst being 0\n",
    "\n",
    "ðŸŽ“ Support: The number of occurrences of each label retrieved\n",
    "\n",
    "ðŸŽ“ Accuracy: (TP + TN)/(TP + TN + FP + FN) The percentage of labels predicted accurately for a sample.\n",
    "\n",
    "ðŸŽ“ Macro Avg: The calculation of the unweighted mean metrics for each label, not taking label imbalance into account.\n",
    "\n",
    "ðŸŽ“ Weighted Avg: The calculation of the mean metrics for each label, taking label imbalance into account by weighting them by their support (the number of true instances for each label).\n",
    "\n",
    "âœ… Can you think which metric you should watch if you want your model to reduce the number of false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Let's say our model is asked to classify pumpkins between two binary categories, category 'orange' and category 'not-orange'.\n",
    "\n",
    "- If your model predicts a pumpkin as not orange and it belongs to category 'not-orange' in reality we call it a true negative, shown by the top left number.\n",
    "- If your model predicts a pumpkin as orange and it belongs to category 'not-orange' in reality we call it a false negative, shown by the bottom left number. \n",
    "- If your model predicts a pumpkin as not orange and it belongs to category 'orange' in reality we call it a false positive, shown by the top right number. \n",
    "- If your model predicts a pumpkin as orange and it belongs to category 'orange' in reality we call it a true positive, shown by the bottom right number.\n",
    "\n",
    "As you might have guessed it's preferable to have a larger number of true positives and true negatives and a lower number of false positives and false negatives, which implies that the model performs better.\n",
    "\n",
    "How does the confusion matrix relate to precision and recall? Remember, the classification report printed above showed precision (0.83) and recall (0.98).\n",
    "\n",
    "Precision = tp / (tp + fp) = 162 / (162 + 33) = 0.8307692307692308\n",
    "\n",
    "Recall = tp / (tp + fn) = 162 / (162 + 4) = 0.9759036144578314\n",
    "\n",
    "âœ… Q: According to the confusion matrix, how did the model do? A: Not too bad; there are a good number of true negatives but also several false negatives. \n",
    "\n",
    "Let's revisit the terms we saw earlier with the help of the confusion matrix's mapping of TP/TN and FP/FN:\n",
    "\n",
    "ðŸŽ“ Precision: TP/(TP + FP) The fraction of relevant instances among the retrieved instances (e.g. which labels were well-labeled)\n",
    "\n",
    "ðŸŽ“ Recall: TP/(TP + FN) The fraction of relevant instances that were retrieved, whether well-labeled or not\n",
    "\n",
    "ðŸŽ“ f1-score: (2 * precision * recall)/(precision + recall) A weighted average of the precision and recall, with best being 1 and worst being 0\n",
    "\n",
    "ðŸŽ“ Support: The number of occurrences of each label retrieved\n",
    "\n",
    "ðŸŽ“ Accuracy: (TP + TN)/(TP + TN + FP + FN) The percentage of labels predicted accurately for a sample.\n",
    "\n",
    "ðŸŽ“ Macro Avg: The calculation of the unweighted mean metrics for each label, not taking label imbalance into account.\n",
    "\n",
    "ðŸŽ“ Weighted Avg: The calculation of the mean metrics for each label, taking label imbalance into account by weighting them by their support (the number of true instances for each label).\n",
    "\n",
    "âœ… Can you think which metric you should watch if you want your model to reduce the number of false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_scores = model.predict_proba(X_test)\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "sns.lineplot([0, 1], [0, 1])\n",
    "sns.lineplot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
